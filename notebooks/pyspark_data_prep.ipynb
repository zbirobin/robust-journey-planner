{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we make some data exploration and data cleaning to have dataframes easier to handle for the tasks. Moreover, we also compute the statistics of the delays, in order to compute later the probability of each route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "username = 'olam'\n",
    "namespace = os.environ['CI_NAMESPACE']\n",
    "project = os.environ['CI_PROJECT']\n",
    "\n",
    "configuration = dict(\n",
    "    name=f\"{username}-{namespace}-{project}\",\n",
    "    executorMemory = \"4G\",\n",
    "    executorCores = 4, \n",
    "    numExecutors = 10,\n",
    "    conf = {\n",
    "        \"spark.jars.repositories\": \"https://repos.spark-packages.org\",\n",
    "        \"spark.jars.packages\": \"graphframes:graphframes:0.7.0-spark2.3-s_2.11\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# set the application name as \"<gaspar_id>-final_assignment\"\n",
    "get_ipython().run_cell_magic('configure', line=\"-f\", cell=json.dumps(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('graphframes_graphframes-0.7.0-spark2.3-s_2.11.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from pyspark.sql.functions import col, length, lit, min, date_format, udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from graphframes import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual data\n",
    "\n",
    "- `BPUIC` is the stop_id (from https://opentransportdata.swiss/de/cookbook/ist-daten/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ORC data into a Spark DataFrame\n",
    "df = spark.read.orc(\"/data/sbb/orc/istdaten/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(n=2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timetable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_times = spark.read.csv(\"/data/sbb/csv/timetable/stop_times/2019/05/07/stop_times.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_times.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_times.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips = spark.read.csv(\"/data/sbb/csv/timetable/trips/2019/05/07/trips.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trips.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar = spark.read.csv(\"/data/sbb/csv/timetable/calendar/2019/05/07/calendar.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.select(\"monday\").withColumn(\"monday\", df_calendar[\"monday\"].cast(IntegerType())).select(F.sum(\"monday\")).show()\n",
    "df_calendar.select(\"tuesday\").withColumn(\"tuesday\", df_calendar[\"tuesday\"].cast(IntegerType())).select(F.sum(\"tuesday\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we cannot conclude that the timetable doesn't depend on the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_calendar.write.parquet('/user/{0}/calendar.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes = spark.read.csv(\"/data/sbb/csv/timetable/routes/2019/05/07/routes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_routes.select('route_id', 'route_desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cound the number of different type of transport\n",
    "df_routes.select('route_desc').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all the possible value for route_desc\n",
    "df_routes.select('route_desc').distinct().show(n=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stations data\n",
    "\n",
    "- Multiple stop_id for same stop (always have a parent, where stop_id finish with P) => keep only 7 first digit and drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations = spark.read.orc('/data/sbb/orc/geostops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.where(length(col('stop_id')) > 7).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Albbruck station\n",
    "df_stations.filter(col('stop_name') == 'Albbruck').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Gaggiolo station\n",
    "df_stations.filter(col('stop_name') == 'Gaggiolo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global var\n",
    "\n",
    "COORD_ZURICH_HB = (47.378177, 8.540192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def compute_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Compute the distance between two coordinates on earth\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers\n",
    "    \n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def rename_stop_id(name):\n",
    "    \"\"\"\n",
    "    Keep only the 7 first digits of the stop_id\n",
    "    \"\"\"\n",
    "    return name[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def distance2walkTime(dist):\n",
    "    \"\"\"\n",
    "    Compute the walking time given the distance.\n",
    "    We assume that the walking speed is 50m/min\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform distance in km to m\n",
    "    dist = 1000 * dist\n",
    "    \n",
    "    # Wakling speed in m/s\n",
    "    walk_speed = 50.0 / 60.0\n",
    "    \n",
    "    # Walk time in seconds\n",
    "    walk_time = dist / walk_speed\n",
    "    \n",
    "    return walk_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def get_type_transport(type_value):\n",
    "    \"\"\"\n",
    "    Get the general transport type, especially for trains\n",
    "    \"\"\"\n",
    "    \n",
    "    list_possible_train = ['TGV', 'Eurocity', 'Standseilbahn', 'Regionalzug', 'RegioExpress', 'S-Bahn',\n",
    "                           'ICE', 'Nacht-Zug', 'Eurostar', 'Schnellzug', 'Intercity', 'InterRegio', 'Extrazug']\n",
    "    \n",
    "    if type_value in list_possible_train:\n",
    "        return 'zug'\n",
    "    else:\n",
    "        return type_value.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual data\n",
    "\n",
    "- Filter data when the date is not in the format of `__.__.____`\n",
    "- Filter when `PRODUKT_ID` is NULL or empty\n",
    "- Filter `DURCHFAHRT_TF`, `ZUSATZFAHRT_TF`, `FAELLT_AUS_TF`\n",
    "- Convert string to timestamp for time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As in HW 2\n",
    "df = df.filter((col('betriebstag').like('__.__.____')) &\n",
    "               (col('produkt_id').isNotNull()) &\n",
    "               (col('durchfahrt_tf') == 'false') &\n",
    "               (col('zusatzfahrt_tf') == 'false') &\n",
    "               (col('faellt_aus_tf') == 'false'))\n",
    "\n",
    "df = df.drop(*['durchfahrt_tf', 'zusatzfahrt_tf', 'faellt_aus_tf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamps instead of strings\n",
    "df = df.withColumn('ankunftszeit', F.to_timestamp('ankunftszeit', 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('abfahrtszeit', F.to_timestamp('abfahrtszeit', 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('an_prognose', F.to_timestamp('an_prognose', 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('ab_prognose', F.to_timestamp('ab_prognose', 'dd.MM.yyyy HH:mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename bcpuic to stop_id\n",
    "df = df.withColumnRenamed('bpuic', 'stop_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct wrong produkt_id\n",
    "df = df.withColumn(\"produkt_id\", F.when(F.col(\"produkt_id\")==\"BUS\", \"Bus\").otherwise(F.col(\"produkt_id\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timetable data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Times\n",
    "\n",
    "- Convert string to timestamp for time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamps instead of strings\n",
    "df_stop_times = df_stop_times.withColumn('arrival_time', F.to_timestamp('arrival_time', 'HH:mm:ss'))\n",
    "df_stop_times = df_stop_times.withColumn('departure_time', F.to_timestamp('departure_time', 'HH:mm:ss'))\n",
    "\n",
    "# Drop useless columns\n",
    "df_stop_times = df_stop_times.drop(*['pickup_type', 'drop_off_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_stop_times.write.parquet('/user/{0}/stop_times.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stations data\n",
    "\n",
    "- Filter out stations outside of 15km rayon from Zurich HB\n",
    "- Keep one `stop_id` for each station\n",
    "- Filter actual data by keeping only data for station from Station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep stations inside 15km rayon from Zurich HB\n",
    "df_stations = df_stations.filter(compute_distance(col('stop_lat'), \n",
    "                                                  col('stop_lon'), \n",
    "                                                  lit(COORD_ZURICH_HB[0]),\n",
    "                                                  lit(COORD_ZURICH_HB[1])) < 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename stop_id \n",
    "df_stations = df_stations.withColumn('stop_id', rename_stop_id('stop_id'))\\\n",
    "                         .dropDuplicates(['stop_id'])\\\n",
    "                         .drop(*['location_type', 'parent_station'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stations.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_stations.write.parquet('/user/{0}/stations_data.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep actual data for stations inside 15km rayon of Zurich HB\n",
    "df = df.join(df_stations, on='stop_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df.write.parquet('/user/{0}/actual_data.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the type of transport for a given route\n",
    "df_routes = df_routes.withColumn('type', get_type_transport('route_desc')).select('route_id', 'type')\n",
    "df_routes.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trips + Type of transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the type of transport for a given trip_id\n",
    "df_trips_type = df_trips.join(df_routes, on='route_id').select('trip_id', 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_trips_type.write.parquet('/user/{0}/trips_type.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Times + Trips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and store a dataframe enabling to have the stop_ids with the service_id\n",
    "stop_times_trips = df_trips.join(df_stop_times[['trip_id', 'arrival_time', 'departure_time', \\\n",
    "                                                             'stop_id', 'stop_sequence']], 'trip_id', 'inner')\n",
    "stop_times_trips = stop_times_trips[['service_id', 'trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_trips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "stop_times_trips.write.parquet('/user/{0}/stop_times_and_trips.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network of stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable\n",
    "MAX_DIST_WALK = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network by walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the rows of a static network connecting all the stations which can be reached by walking from one to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataframe that contains, for each row, the walking distance and time between two stops\n",
    "# such that the two stops are less than 500m far from each other\n",
    "df_edges_walk = df_stations.crossJoin(df_stations.withColumnRenamed('stop_id', 'stop_id2')\\\n",
    "                                                 .withColumnRenamed('stop_lat', 'stop_lat2')\\\n",
    "                                                 .withColumnRenamed('stop_lon', 'stop_lon2'))\\\n",
    "                           .withColumn('distance', compute_distance(col('stop_lat'), col('stop_lon'), col('stop_lat2'), col('stop_lon2')))\\\n",
    "                           .filter((col('stop_id') != col('stop_id2')) &\n",
    "                                   (col('distance')< MAX_DIST_WALK))\\\n",
    "                           .withColumn('travel_time', distance2walkTime(col('distance')))\\\n",
    "                           .select(['stop_id', 'stop_id2', 'distance', 'travel_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edges_walk.printSchema()\n",
    "df_edges_walk.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_edges_walk.write.parquet('/user/{0}/edges_walk.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network by public transportations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the rows of a static network connecting all the stations which can be reached by taking a public transportation from one to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep stop times for stations inside 15km rayon of Zurich HB\n",
    "df_stop_times_zurich = df_stop_times.join(df_stations.select(\"stop_id\").distinct(), on='stop_id', how='inner')\n",
    "df_stop_times_zurich.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that the time between two stops are always the same for the same type of transport\n",
    "df_edges_public_transportations = df_stop_times_zurich.crossJoin(df_stop_times_zurich.withColumnRenamed('stop_id', 'stop_id2')\\\n",
    "                                                     .withColumnRenamed('trip_id', 'trip_id2')\\\n",
    "                                                     .withColumnRenamed('arrival_time', 'arrival_time2')\\\n",
    "                                                     .withColumnRenamed('departure_time', 'departure_time2')\\\n",
    "                                                     .withColumnRenamed('stop_sequence', 'stop_sequence2'))\\\n",
    "                    .filter((col('stop_sequence2') == col('stop_sequence') + 1) & (col('trip_id') == col('trip_id2')))\\\n",
    "                    .dropDuplicates(['stop_id', 'stop_id2'])\\\n",
    "                    .withColumn('travel_time', col('arrival_time2').cast(\"long\") - col('departure_time').cast(\"long\"))\\\n",
    "                    .select('stop_id', 'stop_id2','travel_time')\n",
    "df_edges_public_transportations.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to HDFS the processed data\n",
    "df_edges_public_transportations.write.parquet('/user/{0}/edges_transport.parquet'.format(username))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create statistics Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we compute the statistics of the delays for each route, each time and each different weekday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df.select(\"stop_id\", \"produkt_id\", \"ankunftszeit\", \"an_prognose\")\n",
    "df_stats.show(1)\n",
    "df_stats.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf\n",
    "def weekday(timestamp):\n",
    "    \"\"\"\n",
    "    Return the weekday of the timestamp: 0 == Monday, 1 == Tuesday,...\n",
    "    \"\"\"\n",
    "    if timestamp:\n",
    "        return timestamp.weekday()\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the weekday to the dataframe.\n",
    "df_stats = df_stats.withColumn(\"weekday\", weekday(col(\"ankunftszeit\")))\n",
    "df_stats.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the delay of each route\n",
    "df_stats = df_stats.na.drop()\\\n",
    "                   .withColumn(\"delay\", ((col(\"an_prognose\").cast(\"long\") - col(\"ankunftszeit\").cast(\"long\"))/60).cast(\"int\"))\\\n",
    "                   .drop(\"an_prognose\").cache()\n",
    "df_stats.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have now everything to compute the statistics for each weekday and transportype.\n",
    "\n",
    "idx2weekday = {0: 'monday', 1: 'tuesday', 2: 'wednesday', 3: 'thursday', 4: 'friday', 5: 'saturday', 6: 'sunday'}\n",
    "transport_types = [\"Zug\", \"Tram\", \"Bus\"]\n",
    "\n",
    "for weekday in range(5):\n",
    "    for transport_type in transport_types:\n",
    "        \n",
    "        df_stats_temp = df_stats.filter((col(\"weekday\")== weekday) & (col(\"produkt_id\")==transport_type))\\\n",
    "                                .drop(\"weekday\", \"produkt_id\")\\\n",
    "                                .withColumn(\"arrival_time\", date_format('ankunftszeit', 'HH:mm:ss'))\\\n",
    "                                .drop(\"ankunftszeit\")\\\n",
    "                                .groupBy(\"arrival_time\", \"stop_id\")\\\n",
    "                                .agg(F.collect_list(\"delay\"))\\\n",
    "                                .withColumnRenamed(\"collect_list(delay)\", \"delay_list\")\n",
    "        \n",
    "        # Write to HDFS the data\n",
    "        df_stats_temp.write.parquet('/user/{0}/stats_delays_{1}_{2}.parquet'.format(username, idx2weekday.get(weekday), transport_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats.filter((col(\"weekday\")== 0) & (col(\"produkt_id\")=='Zug'))\\\n",
    "                                .drop(\"weekday\", \"produkt_id\")\\\n",
    "                                .withColumn(\"arrival_time\", date_format('ankunftszeit', 'HH:mm:ss'))\\\n",
    "                                .drop(\"ankunftszeit\")\\\n",
    "                                .groupBy(\"arrival_time\", \"stop_id\")\\\n",
    "                                .agg(F.collect_list(\"delay\"))\\\n",
    "                                .withColumnRenamed(\"collect_list(delay)\", \"delay_list\").show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
